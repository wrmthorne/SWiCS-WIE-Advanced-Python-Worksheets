{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the required libraries for this notebook by running the cell below. If you are using conda, uncomment and run the conda command. If you are using PIP, uncomment and use the PIP command.\n",
    "\n",
    "COME BACK AND MAKE CONDA USE ENVIRONMENT.YML TO LOAD INSTEAD OF REQUIREMENTS.TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!conda install --yes --prefix {sys.prefix} \n",
    "#!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensures that all dataframes are displayed on one line instead of breaking columns across multiple lines\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data\n",
    "\n",
    "For this example, the gold medal results of olympic track and field events will be used to show linear regression in action. The men's marathon times from 1896 to 2016 will be used to draw a line of best fit. The original dataset can be found [here](https://www.kaggle.com/datasets/jayrav13/olympic-track-field-results/discussion/272523)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/wrmthorne/linear-regression/main/olympic_data.csv', encoding= 'unicode_escape')\n",
    "\n",
    "# Print some information about the dataframe\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Extract only the men's marathon results from the full list of data and keep only the 'Year' and 'Result' columns. We are only interested in plotting the year against the time achieved for each year.\n",
    "\n",
    "Hint: The event is called 'Marathon Men'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example answer\n",
    "data = df[['Year', 'Result']].loc[df.Event == 'Marathon Men']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see whether your method returned the right results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Once we have selected the subset of data we want to use, we need to split it into each separate axis to represent. Since we are only mapping the function of two columns, we can use a function of the form:\n",
    "\n",
    "$$\n",
    "y = mx + c\n",
    "$$\n",
    "\n",
    "As our independent variable is the year and our dependent variable is the result, we assign $x$ to year and $y$ to time. Because time is in a string format in the dataset, we need to convert it to a numerical form that can be use in arithmetic. As times are represented in a number of different ways in this dataset, we will use [regular expressions](https://docs.python.org/3/library/re.html) for convert them to an integer number of seconds. If they were all in the same format, we could use a library such as [time](https://docs.python.org/3/library/time.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes string time format and converts time into integer seconds\n",
    "def convert_to_seconds(x):\n",
    "    parsed_x = re.search(r'((?P<hours>\\d+).(?P<minutes>\\d+).(?P<seconds>\\d+))', x)\n",
    "    hours    = int(parsed_x.group('hours')) * 60 * 60\n",
    "    minutes  = int(parsed_x.group('minutes')) * 60\n",
    "    seconds  = int(parsed_x.group('seconds'))\n",
    "    return hours + minutes + seconds\n",
    "\n",
    "# Show y before transformation\n",
    "print(f'Before Transform: {data.Result[:5].values}')\n",
    "\n",
    "# Convert Results column to be in seconds\n",
    "results_transformed = data.Result.apply(lambda x: convert_to_seconds(x))\n",
    "\n",
    "# Show y after transformation\n",
    "print(f'After Transform:  {results_transformed[:5].values}')\n",
    "\n",
    "# Assign x and y and shape them into a column vector\n",
    "x = np.array(data.Year.values)\n",
    "y = np.array(results_transformed.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Data\n",
    "\n",
    "Now that we have our data in a computer interpretable format, we can visualise the data to inspect what we expect the result to look like. We can plot the data on a scatter plot and manully visualise where a line of best fit might pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'rx')\n",
    "plt.title('Gold Medal Times for Men\\'s Olympic Marathon')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Time (Seconds)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Use a numpy linspace with 100 values to draw a line, on the same plot as our x and y data, with the function:\n",
    "\n",
    "$$\n",
    "f(x) = x + 7,000\n",
    "$$\n",
    "\n",
    "Make sure not to overwite x with this so you can plot the original x data with this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(1896, 2016, 100)\n",
    "\n",
    "y_test = x_test + 7000\n",
    "\n",
    "plt.plot(x, y, 'rx')\n",
    "plt.plot(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Fitting the Data\n",
    " \n",
    " You may be able to see that it doesn't fit the data very well. It roughly intersects the data but it doesn't follow the trend of the data well and would make a very poor prediction. This is finally where linear regression comes into play. Linear regression uses linear algebra to mathematically solve for variables $m$ and $c$ in the above formula to minimise the sum of squares error:\n",
    "\n",
    " $$\n",
    " E(m, c) = \\sum^{n}_{i=1}(y_i - (mx_i + c))^2\n",
    " $$\n",
    "\n",
    " If you are interested in the maths of linear regression, it will be covered in the extension section of this notebook. If you're not interested, this section will cover how to use the [lingress](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.linregress.html) method of [SciPy](https://docs.scipy.org/doc/scipy-0.14.0/reference/index.html).\n",
    "\n",
    " SciPy is a collection of mathematical algorithms and functions, built on numpy to enable a user to easily perform complex tasks with simple methods. Although you have less granular control than other libraries which perform a similar task such as [scikit-learn](https://scikit-learn.org/stable/), the wrapper is simple ans easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lin = slope * x + intercept\n",
    "\n",
    "plt.plot(x, y, 'rx')\n",
    "plt.plot(x, y_lin, 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Our Line of Best Fit\n",
    "\n",
    "Although our line of best fit generally covers the points, it would still make a bad prediction of the next olympic's gold medal pace. This is because we have applied a linear function to a non-linear distribution of points. This is a limitation of linear regression in its fundamental form but it can be built upon using basis functions.\n",
    "\n",
    "A basis function $\\phi$ directly transforms the original input space $x$, meaning a set of linear parameters can still be used represented with $m$ and $c$, but a non-linear line of best fit can be represented. Simply put, our function to optimise remains the same, but on a transformed variable:\n",
    "\n",
    "$$\n",
    "y = mx + c \\\\\n",
    "y = m\\phi(x) + c\n",
    "$$\n",
    "\n",
    "Any function can be used as a basis function but a common basis function is polynomial:\n",
    "\n",
    "$$\n",
    "\\phi{_j}(x_i^j) \\\\\n",
    "\\Phi(x) = x + x^2 + x^3 \\dots + x^n\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension: The Maths\n",
    "\n",
    "If you are interested in the maths of linear regression, this section will cover how to implement linear regression manually in numpy. This will also cover the underlying linear algebra involved and generally dig deeper into the topic. If you want every last step in the derivation of all of these formulas, they can be found in [this article](https://towardsdatascience.com/understanding-linear-regression-eaaaed2d983e) and in many other places online.\n",
    "\n",
    "As stated before, the aim of linear regression is to algebraically solve for $m$ and $c$ in a function of the form:\n",
    "\n",
    "$$\n",
    "y = mx + c\n",
    "$$\n",
    "\n",
    "There are two ways of minimising the loss w.r.t $m$ and $c$. It can be solved using linear algebra (the method which will be covered here) or it can be performed iteratively, that is, by updating the values by a incremental amount ([learning rate](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/)) at each iteration of a loop. This is known as [gradient descent](https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21) and is the primary algorithm used on extremely large models where there is too much data to store in memory at any one time to solve algebraically.\n",
    "\n",
    "All standard models of linear regression will make use of matrix multiplication. If you haven't used matrices before, [khan academy](https://www.khanacademy.org/math/precalculus/x9e81a4f98389efdf:matrices/x9e81a4f98389efdf:mat-intro/v/introduction-to-the-matrix) has a really good series on them. We will first cover how to use matrices in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matrix A and matrix B (2D numpy arrays)\n",
    "A = np.random.randint(1, 5, size=(4, 4))\n",
    "B = np.random.randint(1, 5, size=(4, 4))\n",
    "\n",
    "print(A, end='\\n\\n')\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot and outer products of matrices can be performed in numpy simply. Dot product can be performed using np.dot() (or @ but it is very [slightly different](https://stackoverflow.com/questions/34142485/difference-between-numpy-dot-and-python-3-5-matrix-multiplication) in some cases) and the outer product can be calculated with np.outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.dot() and @ are basically equivalent\n",
    "print(np.dot(A, B), end='\\n\\n')\n",
    "print(A @ B, end='\\n\\n')\n",
    "\n",
    "print(np.outer(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices can have their inverse inverse and transpose applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposition\n",
    "print(A, end='\\n\\n')\n",
    "print(A.T, end='\\n\\n')\n",
    "\n",
    "# Inversion\n",
    "print(np.linalg.inv(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now show how summation across a formula is equivalent to matrix multiplication when use in a specific way. If we define a really large vector C and we want to find the sum of the sqares of all elements in C, the same operation can be achieved much faster using matrix multiplication:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n}c_i^2 = C^T \\cdot C\n",
    "$$\n",
    "\n",
    "This difference may not seem important for a simple calculation like this but by using matrix multiplication, we avoid iteration which is a major bottle neck in computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.random.randint(1, 5, size=10000000)\n",
    "\n",
    "# Summation \n",
    "start_time = time.time()\n",
    "summation = sum(C**2)\n",
    "print(f'Summation: {time.time() - start_time:.4f}s')\n",
    "\n",
    "# Matrix multiplication\n",
    "start_time = time.time()\n",
    "mat_mul = np.dot(C.T, C)\n",
    "print(f'Matrix Multiplication: {time.time() - start_time:.4f}s')\n",
    "\n",
    "print(f'Equivalent?: {summation == mat_mul}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have gone through the basics of matrices in numpy, we can start to look at how we can apply this to linear regression. First, we need to understand how we can convert objective function into a vectorised form. We can take our original objective function and stack the two parameters into a weight matrix $\\mathbf{w}$:\n",
    "\n",
    "$$\n",
    " \\mathbf{w} = \\begin{bmatrix} c \\\\ m \\end{bmatrix} \\\\\n",
    "\n",
    "E(\\mathbf{w}) = \\sum_{i=1}^n (y_i - f(\\mathbf{x}_i; \\mathbf{w}))^2\n",
    "$$\n",
    "\n",
    "In this format, we need $x$ to be in the design-matrix format, covered above. i.e.\n",
    "\n",
    "$$\n",
    "x_i = \\begin{bmatrix} 1 \\\\ x_i \\end{bmatrix} \\\\\n",
    "\n",
    "X = \\begin{bmatrix} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "We can now define the function $f(x_i)$:\n",
    "\n",
    "$$\n",
    "f(x_i) = x_i^Tw \\\\\n",
    "f(X;\\mathbf{w}) = X\\mathbf{w}\n",
    "$$\n",
    "\n",
    "Now, using the concept we covered above about matrix multiplication being equivalent to summation when performed in a particular manner, we can produce the following equation:\n",
    "\n",
    "$$\n",
    "E(\\mathbf{w}) = (y - f(X;\\mathbf{w}))^T(y-f(X;\\mathbf{w}))\n",
    "$$\n",
    "\n",
    "If we expand everything out, we can now define our vectorised function of the form:\n",
    "\n",
    "$$\n",
    "E(\\mathbf{w}) = (y - X\\mathbf{w})^T(y-X\\mathbf{w})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to differentiate w.r.t $\\mathbf{w}$ to algebraically solve and find the optimimum (where gradient is 0).\n",
    "\n",
    "$$\n",
    "\\frac{dE\\mathbf{w}}{d\\mathbf{w}} = -2X^Ty + 2X^TX\\mathbf{w}\n",
    "$$\n",
    "\n",
    "Here, we are exploiting the fact that $\\mathbf{w}$ is the composite of $c$ and $m$ to perform the optimisation for both values at the same time:\n",
    "\n",
    "$$\n",
    "\\frac{dE\\mathbf{w}}{d\\mathbf{w}} = \\begin{bmatrix} \\frac{\\partial E(\\mathbf{w})}{\\partial c} \\\\ \\frac{\\partial E(\\mathbf{w})}{\\partial m} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "With all of the heavy maths done, we can return to numpy. Here we will make use of the np.linalg.solve method to perform the linear algebra for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
