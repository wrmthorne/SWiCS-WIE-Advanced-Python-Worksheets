{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Plotting, Advanced Numpy & Linear Regression\n",
    "---\n",
    "Written by Liam Thorne for SWiCS & WiE Python Data-Analysis Sessions (2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Plotting\n",
    "\n",
    "Lots of data that is used in practice isn't necessarily 2-dimensional like many of the contrived examples from before. [matplotlib](https://matplotlib.org/stable/index.html) also covers surface plots and other 3D graphs using a similar interface to the 2D counterparts.\n",
    "\n",
    "To install all the required libraries for this notebook, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!wget https://raw.githubusercontent.com/wrmthorne/SWiCS-WIE-Advanced-Python-Worksheets/main/Advanced%201%20-%20Linear%20Regression/requirements.txt\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Ensures that all dataframes are displayed on one line instead of breaking columns across multiple lines\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common uses of plotting is to compare data against other data. This often requires multiple plots to be aligned together in a grid to allow for comparison. Matplotlib allows this by using a system of [subplots](https://matplotlib.org/3.5.0/api/_as_gen/matplotlib.pyplot.subplots.html). The example below will generate a 2x2 grid of plots which are all aligned with eachother in a grid. The index of each plot can be accessed using `axs[i, j]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "colours = np.array([['red', 'blue'], ['purple', 'green']])\n",
    "\n",
    "for i in range(len(axs[0])):\n",
    "    for j in range(len(axs[1])):\n",
    "        axs[i, j].scatter(np.random.randint(10, size=10), np.random.randint(10, size=10), color=colours[i, j])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - 3x3 Grid\n",
    "\n",
    "Plot 9 graphs in a 3x3 grid. Insert random data into each of the plots except for each of the plots on the diagonal from top left to bottom right. I.e. randomly fill all the graphs with a label $O$:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "- & O & O \\\\\n",
    "O & - & O \\\\\n",
    "O & O & - \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "You don't need to worry about colours but feel free to add them if you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 - Generate some data\n",
    "\n",
    "To explore 3D plots in matplotlib, we first need to create some 3D data. Create 3 variables `x`, `y`, and `z`, each with 100 values generated using a normal distribution. The values should be between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plots\n",
    "Plotting in 3D is very similar to in 2 dimensions in matplotlib. Using the x, y anf z values assigned above, we can plot this data on a [scatter plot](https://matplotlib.org/stable/gallery/mplot3d/scatter3d.html) to visualise the positions of the 100 points in 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(x, y, z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 - Draw the vectors\n",
    "\n",
    "Especially when working with vectors, it can be very useful to show these on a 3D plot. Draw a line on this 3D plot from (0, 0) to the first point described by x, y, and z. Now draw a line from (0, 0) to the second point described by x, y, and z.\n",
    "\n",
    "*Extension*:\n",
    "\n",
    "Too easy? Calculate the angle (in radians or degrees) [between the the two vectors](https://www.omnicalculator.com/math/angle-between-two-vectors) you have just plotted using only numpy and base python functions.\n",
    "\n",
    "Hint: You can use [np.linalg](https://numpy.org/doc/stable/reference/routines.linalg.html) - the linear algebra sub-package for numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface Plots\n",
    "\n",
    "Quite often your data may represent a surface. Normal distributions appear frequently in many applications, especially in fields such as machine-learning. We will now plot a multivariate normal distribution.\n",
    "\n",
    "If you aren't familiar with [multivariate normal distributions](https://www-sigproc.eng.cam.ac.uk/foswiki/pub/Main/PB404/multivariate_normals.pdf), we will briefly cover them here. Univariate normal distributions make use of two parameters, mean ($\\mu$) and variance ($\\sigma^2$). Using just these two parameters, the distribution can be plotted using the following formula:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sigma\\sqrt{2 \\pi}}e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2}\n",
    "$$\n",
    "\n",
    "By defining just $\\mu$ and $\\sigma$, we can define the form function for some variable $x$. Multivariate normal distributions add another dimension with variable $y$. This variable will also have its own mean $\\mu_y$ and variance $\\sigma^2_y$. To plot this data, we need to take into account the [covariance](https://www.investopedia.com/ask/answers/041515/what-difference-between-variance-and-covariance.asp) between the variables, that is the joint variability between $x$ and $y$. This logic can be extended into any number of dimensions with any number of random variables but beyond bivariate distributions, we can no longer visualise them.\n",
    "\n",
    "We will use scipy to calculate the multivariate normal for us using our multi-variate mean $\\begin{bmatrix} \\mu_x \\\\ \\mu_y \\end{bmatrix}$ and covariance $\\begin{bmatrix} \\sigma^2_x & 0 \\\\ 0 & \\sigma^2_y \\end{bmatrix}$\n",
    "\n",
    "### Question 4 - Experiment with each variable and try and predict what will happen to the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our variables for our function\n",
    "mu_x = 0\n",
    "var_x = 20\n",
    "\n",
    "mu_y = 0\n",
    "var_y = 20\n",
    "\n",
    "# Create a set of points to plot\n",
    "x = np.linspace(-20, 20, 100)\n",
    "y = np.linspace(-20, 20, 100)\n",
    "\n",
    "# Define the coordinates of x and y (https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "pos = np.empty(X.shape + (2,))\n",
    "pos[:, :, 0] = X\n",
    "pos[:, :, 1] = Y\n",
    "\n",
    "# Calculate the bivariate distribution using the multivariate mean and covariance\n",
    "dist = stats.multivariate_normal([mu_x, mu_y], [[var_x, 0], [0, var_y]])\n",
    "\n",
    "# Plot from the probability density function\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.set_title(f'Multivariate Distribution (mean: ({mu_x}, {mu_y}))')\n",
    "\n",
    "ax.plot_surface(X, Y, dist.pdf(pos), cmap='plasma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "When dealing with data, particularly from experiments, we may want to fit a line to the distribution to learn the relationship and possibly predict future outcomes with untested variables. The simplest form of this is using linear regression to draw a straight line of best fit for a distribution of data. This section of the notebook will cover how to implement linear regression in a number of situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data\n",
    "\n",
    "For this example, the gold medal results of olympic track and field events will be used to show linear regression in action. The medal winning times for the men's marathon from each olympic games between 1896 and 2016 will be used to draw a line of best fit. The original dataset can be found [here](https://www.kaggle.com/datasets/jayrav13/olympic-track-field-results/discussion/272523)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/wrmthorne/SWiCS-WIE-Advanced-Python-Worksheets/main/Advanced%201%20-%20Linear%20Regression/olympic_data.csv', encoding='unicode_escape')\n",
    "\n",
    "# Print some information about the dataframe\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 - Extracting the correct data\n",
    "\n",
    "Extract only the Women's 100m results from the full list of data and keep only the 'Year' and 'Result' columns. We are only interested in plotting the year against the time achieved for each year. name the variable `data`.\n",
    "\n",
    "Hint: You can get a list of all of the olympic events in this dataset using df.Event.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Now that we have our subset of our data, we can view the dataset. Within the data, we can see that there are some missing values. These will cause problems when trying to plot and define our line of best fit. Normally removing missing values in pandas made simple by using `df.dropna()` but this dataset has the missing values stored as a string `'None'`. We can use a work-around to resolve this. `df.mask` replaces values in a dataframe where a condition is true so we can use it to set the value to `NaN` where `'None'` is present and then run `.dropna()`. More can be found out abour df.mask [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mask.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.mask(df.eq('None')).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only mapping the function of two columns, we can use a function of the form:\n",
    "\n",
    "$$\n",
    "y = mx + c\n",
    "$$\n",
    "\n",
    "As our independent variable is the year and our dependent variable is the result, we assign $x$ to year and $y$ to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by ascending years\n",
    "data = data.sort_values(by=['Year'])\n",
    "\n",
    "# Assign x and y and shape them into a column vector\n",
    "x = np.array(data.Year.values).astype(float)\n",
    "y = np.array(data.Result.values).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Data\n",
    "\n",
    "Now that we have our data in a computer interpretable format, we can visualise the data to inspect what we expect the result to look like. We can plot the data on a scatter plot and manully visualise where a line of best fit might pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'rx')\n",
    "plt.title('Medal Times for Women\\'s Olympic 100m')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Time (Seconds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Fitting the Data\n",
    "\n",
    "This is finally where linear regression comes into play. Linear regression uses linear algebra to mathematically solve for variables $m$ and $c$ in the above formula to minimise the sum of squares error:\n",
    "\n",
    "$$\n",
    "E(m, c) = \\sum^{n}_{i=1}(y_i - (mx_i + c))^2\n",
    "$$\n",
    "\n",
    "We can simplify this whole process of calculating a line of best fit into one line of code using the [stats.lingress](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.linregress.html) method from [SciPy](https://docs.scipy.org/doc/scipy-0.14.0/reference/index.html).\n",
    "\n",
    "If you are interested in the maths of linear regression, it will be covered in the extension section of this notebook where we will manually implement linear regression to better understand how it functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important returned parameters for our function $y = mx + c$ are slope and intercept, representing $m$ and $c$, respectively. r_value represents the [correlation coefficient](https://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/), p_value is a [two-sided p test](https://stattrek.com/regression/slope-test.aspx) and the std_err is the standard error of the estimate, which is of the form:\n",
    "\n",
    "$$\n",
    "SE = \\frac{\\sigma}{\\sqrt{n}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lin = slope * x + intercept\n",
    "\n",
    "plt.plot(x, y, 'rx', label='Data')\n",
    "plt.plot(x, y_lin, 'g-', label='Linear')\n",
    "plt.title('Medal Times for Women\\'s Olympic 100m')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Time (Seconds)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 - Putting it all together\n",
    "\n",
    "Repeat the steps above to plot the data of the `Mens Marathon`. Then calculate and plot the line of best fit.\n",
    "\n",
    "The marathon results are stored in string format in the dataset. We therefore need to convert it to a numerical form that can be use in arithmetic. As times are represented in a number of different ways in this dataset, we will use [regular expressions](https://docs.python.org/3/library/re.html) for convert them to an integer number of seconds. If they were all in the same format, we could use a library such as [time](https://docs.python.org/3/library/time.html).\n",
    "\n",
    "Use `df.apply` to convert all rows into seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes string time format and converts time into integer seconds\n",
    "def convert_to_seconds(x):\n",
    "    parsed_x = re.search(r'((?P<hours>\\d+).(?P<minutes>\\d+).(?P<seconds>\\d+))', x)\n",
    "    hours    = int(parsed_x.group('hours')) * 60 * 60\n",
    "    minutes  = int(parsed_x.group('minutes')) * 60\n",
    "    seconds  = int(parsed_x.group('seconds'))\n",
    "    return hours + minutes + seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Parameters\n",
    "\n",
    "As mentioned before, many real world experiments will not just seek to describe the relationship between two parameters. Instead many will seek to explore 3 or mode parameters to make a prediction. The following work will do just this.\n",
    "\n",
    "The data for this section will come from a spreadsheet with multiple sheets within it. Once again, Pandas comes to the rescue as we make use of inbuilt functions to read in the data and process it further with our models. This example contains 3 sheets where each represents the outcome of the same experiment. A copy of the spreadsheet can be downloaded [here](https://github.com/wrmthorne/linear-regression/blob/main/experiment_data.xlsx) if you wan't to view it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx = pd.ExcelFile('https://raw.githubusercontent.com/wrmthorne/SWiCS-WIE-Advanced-Python-Worksheets/main/Advanced%201%20-%20Linear%20Regression/experiment_data.xlsx')\n",
    "sheet_names = xlsx.sheet_names\n",
    "\n",
    "run1 = pd.read_excel(xlsx, sheet_names[0], index_col=0)\n",
    "run2 = pd.read_excel(xlsx, sheet_names[1], index_col=0)\n",
    "run3 = pd.read_excel(xlsx, sheet_names[2], index_col=0)\n",
    "print(run1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sheet contains 10 rows. Columns a, b, and c represent the independent variables and d is the dependent variable. SciPy is limited to 2 variables when calculating a line of best fit so we will now introduce our second method of calculating a line of best fit using [linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from [sklearn](https://scikit-learn.org/stable/index.html). The sklearn model is slightly mode complex to use but is far more flexible in what it can do.\n",
    "\n",
    "### Question 7 - Preparing the data\n",
    "\n",
    "Once again, we need to prepare out data for regression. For run1, split our independent variables `a`, `b`, and `c` into a numpy array with shape (10, 3) named $X$ and our dependent variable `d` into a numpy array named $y$ with shape (10,). Note here we use capital $X$ to denote a matrix and lower case $y$ to denote a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit our sklearn LinearRegression model on our prepared data. This is also very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = linear_model.LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "slope = lin_reg.coef_\n",
    "intercept = lin_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the benefits of sklearn is that it is built for machine learning so extracting predictions from it is very simple. We can see by plotting our `y` and `y_pred` values that there is a perfect linear relationship between out independent variables and our dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions of our input set\n",
    "y_pred = lin_reg.predict(X)\n",
    "\n",
    "plt.scatter(y, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 - Meta linear regression\n",
    "\n",
    "Use the scipy liner regression tool to draw a line of best fit for this data.\n",
    "\n",
    "Hint: Create a linear space for y and y_pred - it will be useful later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 - Repeat the experiment\n",
    "\n",
    "Create 3 subplots, one for each run. For each run, fit an sklearn model and plot `y` vs. `y_pred`. The use the scipy implementation to draw a line of best fit on the same plot.\n",
    "\n",
    "Do you notice anything special about any of the runs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Line of Best Fit\n",
    "\n",
    "Something to remember is that not all relationships between variables is linear. This is clear from our olympic data from before. One way to tackle this problem is by using a basis function. A basis function $\\phi$ directly transforms the original input space $x$, meaning a set of linear parameters can still be used for $m$ and $c$, but a non-linear line of best fit can be represented. \n",
    "\n",
    "$$\n",
    "y = mx + c\n",
    "$$\n",
    "$$\n",
    "y = m\\phi(x) + c\n",
    "$$\n",
    "\n",
    "Any function can be used as a basis function but a common basis function is polynomial:\n",
    "\n",
    "$$\n",
    "\\phi{_j}(x_i^j)\n",
    "$$\n",
    "$$\n",
    "\\Phi(x) = x + x^2 + x^3 \\dots + x^n\n",
    "$$\n",
    "\n",
    "We will repeat our Women's 100m experiment again, this time using a basis function to draw a line of best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/wrmthorne/SWiCS-WIE-Advanced-Python-Worksheets/main/Advanced%201%20-%20Linear%20Regression/olympic_data.csv', encoding='unicode_escape')\n",
    "data = df[['Year', 'Result']].loc[df.Event == '100M Women'].mask(df.eq('None')).dropna()\n",
    "data = data.sort_values(by=['Year'])\n",
    "\n",
    "# Assign x and y and shape them into a column vector\n",
    "x = np.array(data.Year.values, dtype=float).reshape(-1, 1)\n",
    "y = np.array(data.Result.values, dtype=float).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our polynomial basis function which will convert our data into the desired form $\\Phi$, specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(x, num_basis=4):\n",
    "    Phi = np.zeros((x.shape[0], num_basis))\n",
    "    for i in range(num_basis):\n",
    "        Phi[:, i:i+1] = x**i\n",
    "    return Phi\n",
    "\n",
    "X_basis = polynomial(x)\n",
    "print(X_basis[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = linear_model.LinearRegression()\n",
    "lin_reg.fit(X_basis, y)\n",
    "y_pred = lin_reg.predict(X_basis)\n",
    "\n",
    "plt.plot(x, y, 'rx', label='Data')\n",
    "plt.plot(x, y_pred, label='Prediction')\n",
    "plt.title('Medal Times for Women\\'s Olympic 100m')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Time (Seconds)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 - Num Basis\n",
    "\n",
    "What do you expect would happen if you set the number of basis to 1? What about 2? What about the number of elements in the dataset? \n",
    "\n",
    "Repeat the experiment above and find out. Are the results what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample answer\n",
    "X_basis = polynomial(x, num_basis=len(x))\n",
    "lin_reg = linear_model.LinearRegression()\n",
    "lin_reg.fit(X_basis, y)\n",
    "y_pred = lin_reg.predict(X_basis)\n",
    "\n",
    "plt.plot(x, y, 'rx', label='Data')\n",
    "plt.plot(x, y_pred, label='Prediction')\n",
    "plt.title('Medal Times for Women\\'s Olympic 100m')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Time (Seconds)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension: The Maths\n",
    "\n",
    "There are `no questions` in this section but if you are interested in the maths of linear regression, this section will cover how to implement linear regression manually in numpy. This will also cover the underlying linear algebra involved and generally dig deeper into the topic. If you want every last step in the derivation of all of these formulas, they can be found in [this article](https://towardsdatascience.com/understanding-linear-regression-eaaaed2d983e) and in many other places online.\n",
    "\n",
    "As stated before, the aim of linear regression is to algebraically solve for $m$ and $c$ in a function of the form:\n",
    "\n",
    "$$\n",
    "y = mx + c\n",
    "$$\n",
    "\n",
    "There are two ways of minimising the loss w.r.t $m$ and $c$. It can be solved using linear algebra (the method which will be covered here) or it can be performed iteratively, that is, by updating the values by a incremental amount ([learning rate](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/)) at each iteration of a loop. This is known as [gradient descent](https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21) and is the primary algorithm used on extremely large models where there is too much data to store in memory at any one time to solve algebraically.\n",
    "\n",
    "All standard models of linear regression will make use of matrix multiplication. If you haven't used matrices before, [khan academy](https://www.khanacademy.org/math/precalculus/x9e81a4f98389efdf:matrices/x9e81a4f98389efdf:mat-intro/v/introduction-to-the-matrix) has a really good series on them. If you need a recap on how to use matrices in NumPy, we covered them in our [numpy workshop](https://github.com/wrmthorne/SWiCS-WIE-Intermediate-Python-Worksheets/tree/main/Intermediate%201%20-%20Numpy%20and%20Plotting)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to understand how we can convert objective function into a vectorised form. We can take our original objective function and stack the two parameters into a weight matrix $\\mathbf{w}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\begin{bmatrix} c \\\\ m \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "E(\\mathbf{w}) = \\sum_{i=1}^n (y_i - f(\\mathbf{x}_i; \\mathbf{w}))^2\n",
    "$$\n",
    "\n",
    "In this format, we need $x$ to be in the design-matrix format, covered above. i.e.\n",
    "\n",
    "$$\n",
    "x_i = \\begin{bmatrix} 1 \\\\ x_i \\end{bmatrix} \n",
    "$$\n",
    "$$\n",
    "X = \\begin{bmatrix} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "We can now define the function $f(x_i)$:\n",
    "\n",
    "$$\n",
    "f(x_i) = x_i^Tw\n",
    "$$\n",
    "$$\n",
    "f(X;\\mathbf{w}) = X\\mathbf{w}\n",
    "$$\n",
    "\n",
    "Now, using the concept we covered above about matrix multiplication being equivalent to summation when performed in a particular manner, we can produce the following equation:\n",
    "\n",
    "$$\n",
    "E(\\mathbf{w}) = (y - f(X;\\mathbf{w}))^T(y-f(X;\\mathbf{w}))\n",
    "$$\n",
    "\n",
    "If we expand everything out, we can now define our vectorised function of the form:\n",
    "\n",
    "$$\n",
    "E(\\mathbf{w}) = (y - X\\mathbf{w})^T(y-X\\mathbf{w})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to differentiate w.r.t $\\mathbf{w}$ to algebraically solve and find the optimimum (where gradient is 0).\n",
    "\n",
    "$$\n",
    "\\frac{dE\\mathbf{w}}{d\\mathbf{w}} = -2X^Ty + 2X^TX\\mathbf{w}\n",
    "$$\n",
    "\n",
    "Here, we are exploiting the fact that $\\mathbf{w}$ is the composite of $c$ and $m$ to perform the optimisation for both values at the same time:\n",
    "\n",
    "$$\n",
    "\\frac{dE\\mathbf{w}}{d\\mathbf{w}} = \\begin{bmatrix} \\frac{\\partial E(\\mathbf{w})}{\\partial c} \\\\ \\frac{\\partial E(\\mathbf{w})}{\\partial m} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "When we set this value to 0, we can arrange the equation to be in the following form:\n",
    "\n",
    "$$\n",
    "X^TX\\mathbf{w} = X^T\\mathbf{y}\n",
    "$$\n",
    "\n",
    "With all of the heavy maths done, we can return to numpy. For solving the differential of our objective function w.r.t. $\\mathbf{w}$, we can make use of the [np.linalg.solve](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html) method which solves systems of linear equations (if there is an exact solution) in the form we have now put our equation into. Taking our matrices A and B from above, we can solve for an arbitraty w based on the form:\n",
    "\n",
    "$$\n",
    "A\\mathbf{w} = B\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "A = np.random.randint(0, 10, (4, 4))\n",
    "B = np.random.randint(0, 10, (4, 4))\n",
    "\n",
    "# Automatic linear algebra solving\n",
    "w = np.linalg.solve(A, B)\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting everything together, we will repeat the linear regression example using the olympic data from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign x and y and shape them into a column vector\n",
    "x = np.array(data.Year.values).reshape(-1, 1).astype(float)\n",
    "y = np.array(data.Result.values).reshape(-1, 1).astype(float)\n",
    "\n",
    "# Create design matrix\n",
    "X = np.hstack((np.ones_like(x), x))\n",
    "\n",
    "# Solve system of equations\n",
    "w = np.linalg.solve(np.dot(X.T, X), np.dot(X.T, y))\n",
    "\n",
    "# Plot data and line of best fit\n",
    "plt.plot(x, y, 'rx', label='Data')\n",
    "plt.plot(x, np.dot(X, w), 'g-', label='Linear')\n",
    "plt.title('Medal Times for Women\\'s Olympic 100m')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Time (Seconds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The manual implementation of linear regression is much more flexible than the scipy implementation at the cost of a little greater complexity in understanding. The implementation is closer to what sklearn uses, but once again, writing something yourself gives complete freedom to change it however you need. The numpy model also tends to run faster than both scipy and sklearn, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations to average over\n",
    "num_tests = 1000\n",
    "\n",
    "numpy_times = []\n",
    "for i in range(num_tests):\n",
    "    start_time = time.time()\n",
    "    X = np.hstack((np.ones_like(x), x))\n",
    "    w = np.linalg.solve(np.dot(X.T, X), np.dot(X.T, y))\n",
    "    numpy_times.append(time.time() - start_time)\n",
    "\n",
    "print(f'Mean runtime of numpy over {num_tests} tests: {np.mean(numpy_times):.7f}s')\n",
    "\n",
    "sklearn_times = []\n",
    "for i in range(num_tests):\n",
    "    start_time = time.time()\n",
    "    lin_reg = linear_model.LinearRegression()\n",
    "    lin_reg.fit(x, y)\n",
    "    slope, intercept = lin_reg.coef_, lin_reg.intercept_\n",
    "    sklearn_times.append(time.time() - start_time)\n",
    "\n",
    "print(f'Mean runtime of sklearn over {num_tests} tests: {np.mean(sklearn_times):.7f}s')\n",
    "\n",
    "# Reshape x and y to fit scipy requirements\n",
    "x_scipi = x.reshape(-1)\n",
    "y_scipi = y.reshape(-1)\n",
    "\n",
    "scipy_times = []\n",
    "for i in range(num_tests):\n",
    "    start_time = time.time()\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x_scipi, y_scipi)\n",
    "    scipy_times.append(time.time() - start_time)\n",
    "\n",
    "print(f'Mean runtime of scipy over {num_tests} tests: {np.mean(scipy_times):.7f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove that this is an identical result to the SciPy and sklearn implementations, we can plot them on the same chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones_like(x), x))\n",
    "w = np.linalg.solve(np.dot(X.T, X), np.dot(X.T, y))\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x_scipi, y_scipi)\n",
    "lin_reg = linear_model.LinearRegression()\n",
    "lin_reg.fit(x, y)\n",
    "y_pred = lin_reg.predict(x)\n",
    "\n",
    "plt.plot(x, y, 'rx', label='Data')\n",
    "plt.plot(x, np.dot(X, w), 'g-', label='Numpy')\n",
    "plt.plot(x, (slope * x + intercept), 'o', label='SciPy')\n",
    "plt.plot(x, y_pred, 'y+', label='sklearn')\n",
    "plt.title('Medal Times for Women\\'s Olympic 100m')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Time (Seconds)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enxtend this example to implement the basis functions discussed before, we can slightly modify the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(x, num_basis=4, data_limits=[-1., 1.]):\n",
    "    # Standardise input\n",
    "    centre = np.mean(data_limits)\n",
    "    span = np.ptp(data_limits)    # Get min and max values in an array\n",
    "    z = x - centre\n",
    "    z = 2*z/span\n",
    "\n",
    "    # Calculate Phi(x) function\n",
    "    Phi = np.zeros((x.shape[0], num_basis))\n",
    "    for i in range(num_basis):\n",
    "        Phi[:, i:i+1] = x**i\n",
    "    return Phi\n",
    "\n",
    "num_basis = 3\n",
    "Phi = polynomial(x, num_basis=num_basis, data_limits=[x.min(), x.max()])\n",
    "w_poly = np.linalg.solve(np.dot(Phi.T, Phi), np.dot(Phi.T, y))\n",
    "\n",
    "plt.plot(x, y, 'rx', label='Data')\n",
    "plt.plot(x, np.dot(X, w), 'g-', label='Linear X')\n",
    "plt.plot(x, np.dot(Phi, w_poly), 'b-', label=f'Poly. (Basis {num_basis})')\n",
    "plt.title('Medal Times for Women\\'s Olympic 100m')\n",
    "plt.ylabel('Time (Seconds)')\n",
    "plt.xlabel('Year')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Prediction\n",
    "\n",
    "In 2020, E. Thompson-Herah won the women's 100m with a time of 10.61s. Let's see how our predictions line up with that. Although the error of basis 3 is higher, by the next olympics, a pure linear prediction will be far more inaccurate. In about 100 years, the world record sprint time will drop below 0s which can't happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020\n",
    "\n",
    "true_time = 10.61\n",
    "print(f'True Time: {true_time}s')\n",
    "\n",
    "linear_pred = np.dot(np.array([1, 2020]).T, w)\n",
    "print(f'Linear prediction: {linear_pred.item():.2f}s\\n\\tError: {true_time - linear_pred.item():.2f}s')\n",
    "\n",
    "basis_year = polynomial(np.array([year]), num_basis=num_basis, data_limits=[x.min(), 2020])\n",
    "basis_pred = np.dot(basis_year, w_poly)\n",
    "print(f'Basis {num_basis} prediction: {basis_pred.item():.2f}s\\n\\tError: {true_time - basis_pred.item():.2f}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
